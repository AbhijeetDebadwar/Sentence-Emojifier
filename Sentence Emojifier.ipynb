{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is so cute ‚ù§Ô∏è\n",
      "How dare you ask that üòû\n",
      "do you want to join me for dinner  üç¥\n",
      "I said yes üòÑ\n",
      "she is attractive ‚ù§Ô∏è\n",
      "you suck üòû\n",
      "she smiles a lot üòÑ\n",
      "he is laughing üòÑ\n",
      "she takes forever to get ready  üòû\n",
      "French macaroon is so tasty üç¥\n",
      "we made it üòÑ\n"
     ]
    }
   ],
   "source": [
    "for idx in range(30,41):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'I am so impressed by your dedication to this project' has label index 2, which is emoji üòÑ\n",
      "Label index 2 in one-hot encoding format is [ 0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 80\n",
    "print(f\"Sentence '{X_train[idx]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of abhijeet in the vocabulary is 44366\n",
      "the 256123th word in the vocabulary is  namrata\n"
     ]
    }
   ],
   "source": [
    "# we can check word index as follows\n",
    "\n",
    "word = \"abhijeet\"\n",
    "idx = 256123\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is \", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "\n",
    "    words = sentence.split()\n",
    "\n",
    "    avg = np.zeros(shape=word_to_vec_map[words[0].lower()].shape)\n",
    "    \n",
    "    total = avg\n",
    "    for w in words:\n",
    "        total += word_to_vec_map[w.lower()]\n",
    "    avg = total/len(words)\n",
    "    \n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Defining number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    \n",
    "    # Initializing parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Converting Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): \n",
    "        for i in range(m):         \n",
    "            \n",
    "            # Averaging over the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i],word_to_vec_map)     \n",
    "            \n",
    "            # Forward propagating the avg through the softmax layer\n",
    "            z = np.dot(W,avg)+b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Computing cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -(np.sum(Y_oh[i]*np.log(a)))\n",
    "            \n",
    "            # Computing the gradients for backprop\n",
    "            # I directly used the formulas for gradients from Internet\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Updating parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,)\n",
      "(132,)\n",
      "(132, 5)\n",
      "never talk to me again\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "(132, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.95204988128\n",
      "Accuracy: 0.348484848485\n",
      "Epoch: 100 --- cost = 0.0797181872601\n",
      "Accuracy: 0.931818181818\n",
      "Epoch: 200 --- cost = 0.0445636924368\n",
      "Accuracy: 0.954545454545\n",
      "Epoch: 300 --- cost = 0.0343226737879\n",
      "Accuracy: 0.969696969697\n",
      "[[ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 4.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 0.]\n",
      " [ 2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.977272727273\n",
      "Test set:\n",
      "Accuracy: 0.857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.833333333333\n",
      "\n",
      "i adore you ‚ù§Ô∏è\n",
      "i love you ‚ù§Ô∏è\n",
      "funny lol üòÑ\n",
      "lets play with a ball ‚öæ\n",
      "food is ready üç¥\n",
      "not feeling happy üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the above model we can see that, the last example is missclassified \n",
    "# the reason behind it is that, the model consist of only one layer\n",
    "# to increase the accuracy i have developed the model using two LSTM layered model in keras framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initializing X_indices as a numpy matrix of zeros and the correcting shape\n",
    "    X_indices = np.zeros(shape=(m,max_len))\n",
    "    \n",
    "    for i in range(m):                             \n",
    "        \n",
    "        # Converting the ith training sentence in lower case and splitting it into words\n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        # Looping over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            \n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            \n",
    "            # Incrementing j to j + 1\n",
    "            j = j+1\n",
    "\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[ 155345.  225122.       0.       0.       0.]\n",
      " [ 220930.  286375.   69714.       0.       0.]\n",
      " [ 151204.  192973.  302254.  151349.  394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "\n",
    "    vocab_len = len(word_to_index) + 1                 \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]    \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Setting each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Defining Keras embedding layer with the correct output/input sizes, make it trainable. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "\n",
    "    # Building the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Setting the weights of the embedding layer to the embedding matrix.\n",
    "    # our layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    " \n",
    ".\n",
    "    sentence_indices = Input(input_shape, dtype = 'int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    X = Dense(5)(X)\n",
    "\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 0s - loss: 1.5917 - acc: 0.2424     \n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s - loss: 1.5182 - acc: 0.3788     \n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s - loss: 1.4855 - acc: 0.3258     \n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s - loss: 1.4242 - acc: 0.3788     \n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s - loss: 1.3238 - acc: 0.4924     \n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s - loss: 1.2105 - acc: 0.5530     \n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s - loss: 1.1042 - acc: 0.5530     \n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s - loss: 0.9807 - acc: 0.7045     \n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s - loss: 0.8886 - acc: 0.6212     \n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s - loss: 0.7661 - acc: 0.7500     \n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s - loss: 0.7287 - acc: 0.7500     \n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s - loss: 0.5600 - acc: 0.7955     \n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s - loss: 0.5632 - acc: 0.8106     \n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s - loss: 0.4716 - acc: 0.8636     \n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s - loss: 0.4485 - acc: 0.7803     \n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s - loss: 0.4708 - acc: 0.8030     \n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s - loss: 0.4645 - acc: 0.8258     \n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s - loss: 0.6127 - acc: 0.7500     \n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s - loss: 0.4527 - acc: 0.8030     \n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s - loss: 0.4008 - acc: 0.8636     \n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s - loss: 0.3682 - acc: 0.8712     \n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s - loss: 0.2981 - acc: 0.9015     \n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s - loss: 0.2806 - acc: 0.9167     \n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s - loss: 0.3072 - acc: 0.8939     \n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s - loss: 0.2655 - acc: 0.9167     \n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s - loss: 0.2217 - acc: 0.8939     \n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s - loss: 0.2334 - acc: 0.9167     \n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s - loss: 0.2112 - acc: 0.9242     \n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s - loss: 0.2198 - acc: 0.9167     \n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s - loss: 0.2118 - acc: 0.9242     \n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s - loss: 0.1349 - acc: 0.9470     \n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s - loss: 0.1310 - acc: 0.9697     \n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s - loss: 0.1711 - acc: 0.9394     \n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s - loss: 0.1186 - acc: 0.9621     \n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s - loss: 0.1236 - acc: 0.9470     \n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s - loss: 0.1346 - acc: 0.9545     \n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s - loss: 0.1806 - acc: 0.9545     \n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s - loss: 0.1887 - acc: 0.9394     \n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s - loss: 0.1717 - acc: 0.9470     \n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s - loss: 0.1469 - acc: 0.9470     \n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s - loss: 0.0796 - acc: 0.9621     \n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s - loss: 0.0523 - acc: 0.9848     \n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s - loss: 0.0597 - acc: 0.9773     \n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s - loss: 0.0367 - acc: 0.9924     \n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s - loss: 0.0290 - acc: 1.0000     \n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s - loss: 0.0242 - acc: 1.0000     \n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s - loss: 0.0233 - acc: 1.0000     \n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s - loss: 0.0153 - acc: 1.0000     \n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s - loss: 0.0148 - acc: 1.0000     \n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s - loss: 0.0126 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a869b1240>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/56 [================>.............] - ETA: 0s\n",
      "Test accuracy =  0.803571420056\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòÑ prediction: he got a very nice raise\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: she got me a nice present\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is hard\tüòÑ\n",
      "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
      "Expected emoji:üç¥ prediction: any suggestions for dinner\tüòÑ\n",
      "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: My life is so boring\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: go away\t‚öæ\n",
      "Expected emoji:üç¥ prediction: I did not have breakfast ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "# mislabelled examples\n",
    "\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòû\n"
     ]
    }
   ],
   "source": [
    "# predicting emoji over a sentence\n",
    "\n",
    "x_test = np.array([\"not feeling happy\"])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "for x in x_test:\n",
    "    print(x +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
